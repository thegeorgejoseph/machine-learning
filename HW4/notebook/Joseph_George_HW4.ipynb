{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809e6a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import sklearn as sks\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c413c",
   "metadata": {},
   "source": [
    "## Getting the Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8518f15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances 88\n",
      "Number of testing instances 19\n",
      "Number of training instances 69\n"
     ]
    }
   ],
   "source": [
    "#sitting dataset 8 row 54\n",
    "folders = {\n",
    "    'bending1': 7,\n",
    "    'bending2': 6,\n",
    "    'cycling': 15,\n",
    "    'lying' : 15,\n",
    "    'sitting': 15,\n",
    "    'standing': 15,\n",
    "    'walking': 15  \n",
    "}\n",
    "testing_paths = []\n",
    "training_paths = []\n",
    "ordered_paths = []\n",
    "for activity, _max in folders.items():\n",
    "    for i in range(1, _max + 1):\n",
    "        ordered_paths.append((f'../data/{activity}/dataset{i}.csv', activity))\n",
    "        if i <= 2:\n",
    "            testing_paths.append(f'../data/{activity}/dataset{i}.csv')    \n",
    "        elif i == 3 and activity not in ['bending1', 'bending2']:\n",
    "            testing_paths.append(f'../data/{activity}/dataset{i}.csv')\n",
    "        else:\n",
    "            training_paths.append(f'../data/{activity}/dataset{i}.csv')\n",
    "            \n",
    "columns = ['avg_rss12', 'var_rss12', 'avg_rss13', 'var_rss13', 'avg_rss23', 'var_rss23']\n",
    "testing_dfs = [pd.read_csv(path, skiprows = 4).iloc[:, 1:] for path in testing_paths]\n",
    "training_dfs = []\n",
    "for i, path in enumerate(training_paths):\n",
    "    if path == f'../data/bending2/dataset4.csv':\n",
    "        df = pd.read_csv(f'../data/bending2/dataset4.csv', skiprows = 5, delimiter = ' ', index_col = False, names = ['time','avg_rss12','var_rss12','avg_rss13','var_rss13','avg_rss23','var_rss23']).iloc[:, 1:]\n",
    "    else:\n",
    "        df = pd.read_csv(path, skiprows=4).iloc[:, 1:]\n",
    "#         if i == 37:\n",
    "#             print(df.describe())\n",
    "    training_dfs.append(df)\n",
    "\n",
    "\n",
    "print(f\"Number of instances {len(testing_dfs) + len(training_dfs)}\")\n",
    "print(f\"Number of testing instances {len(testing_dfs)}\")\n",
    "print(f\"Number of training instances {len(training_dfs)}\")\n",
    "\n",
    "# for i, df in enumerate(training_dfs):\n",
    "#     print(i, df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06585e71",
   "metadata": {},
   "source": [
    "# 2. Time Series Classification Part 2: Binary and Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfee5eb",
   "metadata": {},
   "source": [
    "## (a) Binary Classification Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b58a1",
   "metadata": {},
   "source": [
    "### i. Assume that you want to use the training set to classify bending from other activities, i.e. you have a binary classification problem. Depict scatter plots of the features you specified in 1(c)iv extracted from time series 1, 2, and 6 of each instance, and use color to distinguish bending vs. other activities. (See p. 129 of the textbook).4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6787ba",
   "metadata": {},
   "source": [
    "cols = ['Mean_1', 'Q1_1', 'Q3_1', 'Mean_2', 'Q1_2', 'Q3_2', 'Mean_6', 'Q1_6', 'Q3_6', 'Label']\n",
    "column_nums = [1,2,6] # take col - 1 for proper indexing\n",
    "rows = []\n",
    "for df, path in zip(training_dfs, training_paths):\n",
    "    label = path.split('/')[2]\n",
    "    if 'bending' in label:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    row = []\n",
    "    for val in column_nums:\n",
    "        row.append(df[df.columns[val-1]].astype(float).mean())\n",
    "        row.append(df[df.columns[val-1]].astype(float).quantile(0.25))\n",
    "        row.append(df[df.columns[val-1]].astype(float).quantile(0.75))\n",
    "    row.append(label)\n",
    "    rows.append(row)\n",
    "scatter_df = pd.DataFrame(rows, columns = cols)\n",
    "sns.pairplot(scatter_df, vars = scatter_df.columns[:-1], hue=\"Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfba3d2",
   "metadata": {},
   "source": [
    "### ii. Break each time series in your training set into two (approximately) equal length time series. Now instead of 6 time series for each of the training instances, you have 12 time series for each training instance. Repeat the experiment in 2(a)i, i.e depict scatter plots of the features extracted from both parts of the time series 1,2, and 6. Do you see any considerable difference in the results with those of 2(a)i?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb5f93",
   "metadata": {},
   "source": [
    "cols = ['Mean_1_first', 'Mean_1_second', 'Q1_1_first', 'Q1_1_second', 'Q3_1_first', 'Q3_1_second', 'Mean_2_first', 'Mean_2_second', 'Q1_2_first', 'Q1_2_second', 'Q3_2_first', 'Q3_2_second', 'Mean_6_first', 'Mean_6_second', 'Q1_6_first', 'Q1_6_second', 'Q3_6_first', 'Q3_6_second', 'Label']\n",
    "column_nums = [1,2,6] # take col - 1 for proper indexing\n",
    "rows = []\n",
    "for df, path in zip(training_dfs, training_paths):\n",
    "    label = path.split('/')[2]\n",
    "    half = len(df) // 2\n",
    "    if 'bending' in label:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    row = []\n",
    "    for val in column_nums:\n",
    "        current = np.reshape(df.iloc[:,val-1].astype(float).values, (-1,1))\n",
    "        row.append(current[:half].mean())\n",
    "        row.append(current[half:].mean())\n",
    "        row.append(np.percentile(current[:half], 25, method = 'midpoint'))\n",
    "        row.append(np.percentile(current[half:], 25, method = 'midpoint'))\n",
    "        row.append(np.percentile(current[:half], 75, method = 'midpoint'))\n",
    "        row.append(np.percentile(current[half:], 75, method = 'midpoint'))\n",
    "    row.append(float(label))\n",
    "    rows.append(row)\n",
    "\n",
    "scatter_df = pd.DataFrame(rows, columns = cols)\n",
    "sns.pairplot(scatter_df, vars = scatter_df.columns[:-1], hue=\"Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7a895",
   "metadata": {},
   "source": [
    "### iii. Break each time series in your training set into l ∈ {1, 2, . . . , 20} time series of approximately equal length and use logistic regression5 to solve the binary classification problem, using time-domain features. Remember that breaking each of the time series does not change the number of instances. It only changes the number of features for each instance. Calculate the p-values for your logistic regression parameters in each model corresponding to each value of l and refit a logistic regression model using your pruned set of features.6 Alternatively, you can use backward selection using sklearn.feature selection or glm in R. Use 5-fold cross-validation to determine the best value of the pair (l, p), where p is the number of features used in recursive feature elimination. Explain what the right way and the wrong way are to perform cross-validation in this problem.7 Obviously, use the right way! Also, you may encounter the problem of class imbalance, which may make some of your folds not having any instances of the rare class. In such a case, you can use stratified cross validation. Research what it means and use it if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64dd4f1",
   "metadata": {},
   "source": [
    "column_nums = [1,2,6] # take col - 1 for proper indexing\n",
    "best_score = float('-inf') \n",
    "for l in range(1, 21):\n",
    "    rows = []\n",
    "    for df, path in zip(training_dfs, training_paths):\n",
    "        size = math.ceil(len(df) / l)\n",
    "        label = path.split('/')[2]\n",
    "        if 'bending' in label:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        row = []\n",
    "        column_names = []\n",
    "        for val in column_nums:\n",
    "            start = 0\n",
    "            part = 1\n",
    "#             current = np.reshape(df.iloc[:,val-1].astype(float).values, (-1,1))\n",
    "            while start < len(df):\n",
    "#                 row.append(current[start:start+size].mean())\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).mean())\n",
    "                column_names.append(f'Mean_{val}_{part}')\n",
    "#                 row.append(np.percentile(current[start:start+size], 25, method = 'midpoint'))\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.25))\n",
    "                column_names.append(f'Q1_{val}_{part}')\n",
    "#                 row.append(np.percentile(current[start: start + size], 75, method = 'midpoint'))\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.75))\n",
    "                column_names.append(f'Q3_{val}_{part}')\n",
    "                start = start + size\n",
    "                part += 1\n",
    "        row.append(label)\n",
    "        rows.append(row[:])\n",
    "        \n",
    "    column_names.append('Label')\n",
    "    logr_df = pd.DataFrame(rows, columns = column_names)\n",
    "    logit = LogisticRegression()\n",
    "    rfecv = RFECV(estimator=logit,cv=StratifiedKFold(5))\n",
    "    model = rfecv.fit(logr_df.iloc[:,:-1], logr_df.iloc[:,-1].astype(int))\n",
    "    if max(model.cv_results_['mean_test_score']) > best_score:\n",
    "        best_score = max(model.cv_results_['mean_test_score'])\n",
    "        best_model = model\n",
    "        best_params = model.get_feature_names_out()\n",
    "        best_l = l\n",
    "        best_train_df = logr_df.copy()\n",
    "\n",
    "print(f'Best l is {best_l}')\n",
    "print(f'Best params are {best_params}')\n",
    "print(max(best_model.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221eda98",
   "metadata": {},
   "source": [
    "### iv. Report the confusion matrix and show the ROC and AUC for your classifier on train data. Report the parameters of your logistic regression βi’s as well as the p-values associated with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccba4c6",
   "metadata": {},
   "source": [
    "y_train_best = best_train_df.iloc[:,-1]\n",
    "X_train_best = best_train_df[best_params].copy()\n",
    "pred = best_model.predict(best_train_df.iloc[:,:-1])\n",
    "cm=metrics.confusion_matrix(y_train_best,pred)\n",
    "print(cm)\n",
    "metrics.plot_roc_curve(best_model, best_train_df.iloc[:,:-1], best_train_df.iloc[:,-1])\n",
    "log_reg = sm.Logit(y_train_best, X_train_best).fit(method='lbfgs')\n",
    "print(log_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b9319",
   "metadata": {},
   "source": [
    "### v. Test the classifier on the test set. Remember to break the time series in your test set into the same number of time series into which you broke your training set. Remember that the classifier has to be tested using the features extracted from the test set. Compare the accuracy on the test set with the cross-validation accuracy you obtained previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8519844f",
   "metadata": {},
   "source": [
    "rows = []\n",
    "for df, path in zip(testing_dfs, testing_paths):\n",
    "    size = math.ceil(len(df) / best_l)\n",
    "    label = path.split('/')[2]\n",
    "    if 'bending' in label:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    row = []\n",
    "    column_names = []\n",
    "    for val in column_nums:\n",
    "        start = 0\n",
    "        part = 1\n",
    "        current = np.reshape(df.iloc[:,val-1].astype(float).values, (-1,1))\n",
    "        while start < len(df):\n",
    "#                 row.append(current[start:start+size].mean())\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).mean())\n",
    "            column_names.append(f'Mean_{val}_{part}')\n",
    "#                 row.append(np.percentile(current[start:start+size], 25, method = 'midpoint'))\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.25))\n",
    "            column_names.append(f'Q1_{val}_{part}')\n",
    "#                 row.append(np.percentile(current[start: start + size], 75, method = 'midpoint'))\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.75))\n",
    "            column_names.append(f'Q3_{val}_{part}')\n",
    "            start = start + size\n",
    "            part += 1\n",
    "    row.append(label)\n",
    "    rows.append(row[:])\n",
    "\n",
    "column_names.append('Label')\n",
    "test_df = pd.DataFrame(rows, columns = column_names)\n",
    "test_df.shape\n",
    "\n",
    "X_test_df = test_df.iloc[:,:-1]\n",
    "y_test_df = test_df.iloc[:, -1]\n",
    "preds = best_model.predict(X_test_df)\n",
    "score = best_model.score(X_test_df, y_test_df)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b2e2fa",
   "metadata": {},
   "source": [
    "### vi. Do your classes seem to be well-separated to cause instability in calculating logistic regression parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df759bce",
   "metadata": {},
   "source": [
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971eee4",
   "metadata": {},
   "source": [
    "### vii. From the confusion matrices you obtained, do you see imbalanced classes? If yes, build a logistic regression model based on case-control sampling and adjust its parameters. Report the confusion matrix, ROC, and AUC of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b363f0",
   "metadata": {},
   "source": [
    "cm=metrics.confusion_matrix(y_test_df,preds)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eebb104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best l is 13\n",
      "Best params are ['Q3_1_1' 'Q1_1_4' 'Q1_1_5' 'Q3_1_5' 'Q3_1_6' 'Q3_1_7' 'Q1_1_8' 'Q1_2_6'\n",
      " 'Q3_2_6' 'Q1_6_1' 'Mean_6_9' 'Q3_6_9' 'Mean_6_10' 'Q3_6_10' 'Mean_6_12'\n",
      " 'Q3_6_12' 'Mean_6_13' 'Q3_6_13']\n",
      "0.975\n"
     ]
    }
   ],
   "source": [
    "column_nums = [1,2,6] # take col - 1 for proper indexing\n",
    "cc_best_score = float('-inf')\n",
    "oversample = SMOTE()\n",
    "for l in range(1, 21):\n",
    "    rows = []\n",
    "    for df, path in zip(training_dfs, training_paths):\n",
    "        size = math.ceil(len(df) / l)\n",
    "        label = path.split('/')[2]\n",
    "        if 'bending' in label:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        row = []\n",
    "        column_names = []\n",
    "        for val in column_nums:\n",
    "            start = 0\n",
    "            part = 1\n",
    "            while start < len(df):\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).mean())\n",
    "                column_names.append(f'Mean_{val}_{part}')\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.25))\n",
    "                column_names.append(f'Q1_{val}_{part}')\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.75))\n",
    "                column_names.append(f'Q3_{val}_{part}')\n",
    "                start = start + size\n",
    "                part += 1\n",
    "        row.append(label)\n",
    "        rows.append(row[:])\n",
    "        \n",
    "    column_names.append('Label')\n",
    "    logr_df = pd.DataFrame(rows, columns = column_names)\n",
    "    logit = LogisticRegression()\n",
    "    rfecv = RFECV(estimator=logit,cv=StratifiedKFold(5))\n",
    "    X, y = oversample.fit_resample(logr_df.iloc[:,:-1], logr_df.iloc[:,-1])\n",
    "    model = rfecv.fit(X,y)\n",
    "    if max(model.cv_results_['mean_test_score']) > cc_best_score:\n",
    "        cc_best_score = max(model.cv_results_['mean_test_score'])\n",
    "        cc_best_model = model\n",
    "        cc_best_params = model.get_feature_names_out()\n",
    "        cc_best_l = l\n",
    "        cc_best_train_df = X.copy()\n",
    "\n",
    "print(f'Best l is {cc_best_l}')\n",
    "print(f'Best params are {cc_best_params}')\n",
    "print(max(cc_best_model.cv_results_['mean_test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b862a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8421052631578947\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for df, path in zip(testing_dfs, testing_paths):\n",
    "    size = math.ceil(len(df) / cc_best_l)\n",
    "    label = path.split('/')[2]\n",
    "    if 'bending' in label:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    row = []\n",
    "    column_names = []\n",
    "    for val in column_nums:\n",
    "        start = 0\n",
    "        part = 1\n",
    "        current = np.reshape(df.iloc[:,val-1].astype(float).values, (-1,1))\n",
    "        while start < len(df):\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).mean())\n",
    "            column_names.append(f'Mean_{val}_{part}')\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.25))\n",
    "            column_names.append(f'Q1_{val}_{part}')\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.75))\n",
    "            column_names.append(f'Q3_{val}_{part}')\n",
    "            start = start + size\n",
    "            part += 1\n",
    "    row.append(label)\n",
    "    rows.append(row[:])\n",
    "\n",
    "column_names.append('Label')\n",
    "test_df = pd.DataFrame(rows, columns = column_names)\n",
    "test_df.shape\n",
    "\n",
    "X_test_df = test_df.iloc[:,:-1]\n",
    "y_test_df = test_df.iloc[:, -1]\n",
    "preds = cc_best_model.predict(X_test_df)\n",
    "score = cc_best_model.score(X_test_df, y_test_df)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ecdacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8421052631578947\n",
      "[[14  1]\n",
      " [ 2  2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7ff3c05dcdc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = cc_best_model.predict(X_test_df)\n",
    "score = cc_best_model.score(X_test_df, y_test_df)\n",
    "print(score)\n",
    "cm=metrics.confusion_matrix(y_test_df,preds)\n",
    "print(cm)\n",
    "metrics.plot_roc_curve(cc_best_model,X_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1cf172",
   "metadata": {},
   "source": [
    "## (b) Binary Classification Using L1-penalized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec976f54",
   "metadata": {},
   "source": [
    "### i. Repeat 2(a)iii using L1-penalized logistic regression,8 i.e. instead of using p- values for variable selection, use L1 regularization. Note that in this problem, you have to cross-validate for both l, the number of time series into which you break each of your instances, and λ, the weight of L1 penalty in your logistic regression objective function (or C, the budget). Packages usually perform cross-validation for λ automatically.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f565a",
   "metadata": {},
   "source": [
    "column_nums = [1,2,6] # take col - 1 for proper indexing\n",
    "l1_best_score = float('-inf') \n",
    "for l in range(1, 21):\n",
    "    rows = []\n",
    "    for df, path in zip(training_dfs, training_paths):\n",
    "        size = math.ceil(len(df) / l)\n",
    "        label = path.split('/')[2]\n",
    "        if 'bending' in label:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        row = []\n",
    "        column_names = []\n",
    "        for val in column_nums:\n",
    "            start = 0\n",
    "            part = 1\n",
    "            while start < len(df):\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).mean())\n",
    "                column_names.append(f'Mean_{val}_{part}')\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.25))\n",
    "                column_names.append(f'Q1_{val}_{part}')\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.75))\n",
    "                column_names.append(f'Q3_{val}_{part}')\n",
    "                start = start + size\n",
    "                part += 1\n",
    "        row.append(label)\n",
    "        rows.append(row[:])\n",
    "        \n",
    "    column_names.append('Label')\n",
    "    logr_df = pd.DataFrame(rows, columns = column_names)\n",
    "    logit = LogisticRegressionCV(solver='liblinear',penalty='l1', cv=5, random_state=0)\n",
    "    model = logit.fit(logr_df.iloc[:,:-1], logr_df.iloc[:,-1].astype(int))\n",
    "    score = model.score(logr_df.iloc[:,:-1], logr_df.iloc[:,-1].astype(int))\n",
    "    if score > l1_best_score:\n",
    "        l1_best_score = score\n",
    "        l1_best_model = model\n",
    "#         l1_best_params = model.get_feature_names_out()\n",
    "        l1_best_c = model.C_\n",
    "        l1_best_l = l\n",
    "        l1_best_train_df = logr_df.copy()\n",
    "\n",
    "print(f'Best l is {l1_best_l}')\n",
    "print(f'Best Cs are {l1_best_c}')\n",
    "print(l1_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a740e1b",
   "metadata": {},
   "source": [
    "### ii. Compare the L1-penalized with variable selection using p-values. Which one performs better? Which one is easier to implement?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206be916",
   "metadata": {},
   "source": [
    "l1 better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99fac1c",
   "metadata": {},
   "source": [
    "## (c) Multi-class Classification (The Realistic Case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9288bb24",
   "metadata": {},
   "source": [
    "### i. Find the best l in the same way as you found it in 2(b)i to build an L1- penalized multinomial regression model to classify all activities in your train- ing set.10 Report your test error. Research how confusion matrices and ROC curves are defined for multiclass classification and show them for this problem if possible.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c329fb",
   "metadata": {},
   "source": [
    "column_nums = [1,2,6] # take col - 1 for proper indexing\n",
    "multi_best_score = float('-inf') \n",
    "for l in range(1, 21):\n",
    "    rows = []\n",
    "    for df, path in zip(training_dfs, training_paths):\n",
    "        size = math.ceil(len(df) / l)\n",
    "        label = path.split('/')[2]\n",
    "        row = []\n",
    "        column_names = []\n",
    "        for val in column_nums:\n",
    "            start = 0\n",
    "            part = 1\n",
    "            while start < len(df):\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).mean())\n",
    "                column_names.append(f'Mean_{val}_{part}')\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.25))\n",
    "                column_names.append(f'Q1_{val}_{part}')\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.75))\n",
    "                column_names.append(f'Q3_{val}_{part}')\n",
    "                start = start + size\n",
    "                part += 1\n",
    "        row.append(label)\n",
    "        rows.append(row[:])\n",
    "    \n",
    "    column_names.append('Label')\n",
    "    logr_df = pd.DataFrame(rows, columns = column_names)\n",
    "    logit = LogisticRegressionCV(solver='saga',penalty='l1', cv=5, random_state=0)\n",
    "    model = logit.fit(logr_df.iloc[:,:-1], logr_df.iloc[:,-1])\n",
    "    score = model.score(logr_df.iloc[:,:-1], logr_df.iloc[:,-1])\n",
    "    if score > multi_best_score:\n",
    "        multi_best_score = score\n",
    "        multi_best_model = model\n",
    "        multi_best_c = model.C_\n",
    "        multi_best_l = l\n",
    "        multi_best_train_df = logr_df.copy()\n",
    "\n",
    "print(f'Best l is {multi_best_l}')\n",
    "print(f'Best Cs are {multi_best_c}')\n",
    "print(multi_best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b000680b",
   "metadata": {},
   "source": [
    "#Test Data\n",
    "rows = []\n",
    "for df, path in zip(testing_dfs, testing_paths):\n",
    "    size = math.ceil(len(df) / multi_best_l)\n",
    "    label = path.split('/')[2]\n",
    "    row = []\n",
    "    column_names = []\n",
    "    for val in column_nums:\n",
    "        start = 0\n",
    "        part = 1\n",
    "        current = np.reshape(df.iloc[:,val-1].astype(float).values, (-1,1))\n",
    "        while start < len(df):\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).mean())\n",
    "            column_names.append(f'Mean_{val}_{part}')\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.25))\n",
    "            column_names.append(f'Q1_{val}_{part}')\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.75))\n",
    "            column_names.append(f'Q3_{val}_{part}')\n",
    "            start = start + size\n",
    "            part += 1\n",
    "    row.append(label)\n",
    "    rows.append(row[:])\n",
    "\n",
    "column_names.append('Label')\n",
    "multi_test_df = pd.DataFrame(rows, columns = column_names)\n",
    "multi_test_df.shape\n",
    "\n",
    "X_test_df_multi = multi_test_df.iloc[:,:-1]\n",
    "y_test_df_multi = multi_test_df.iloc[:, -1]\n",
    "preds = multi_best_model.predict(X_test_df_multi)\n",
    "score = multi_best_model.score(X_test_df_multi, y_test_df_multi)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e62317",
   "metadata": {},
   "source": [
    "confusion = metrics.confusion_matrix(y_test_df_multi, preds)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "# metrics.plot_roc_curve(multi_best_model, multi_best_train_df.iloc[:,:-1], multi_best_train_df.iloc[:,-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4fa98a",
   "metadata": {},
   "source": [
    "### ii. Repeat 2(c)i using a Na ̈ıve Bayes’ classifier. Use both Gaussian and Multi- nomial priors and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27654352",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_nums = [1,2,6] # take col - 1 for proper indexing\n",
    "naive_best_score = float('-inf') \n",
    "for l in range(1, 21):\n",
    "    rows = []\n",
    "    for df, path in zip(training_dfs, training_paths):\n",
    "        size = math.ceil(len(df) / l)\n",
    "        label = path.split('/')[2]\n",
    "        row = []\n",
    "        column_names = []\n",
    "        for val in column_nums:\n",
    "            start = 0\n",
    "            part = 1\n",
    "            while start < len(df):\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).mean())\n",
    "                column_names.append(f'Mean_{val}_{part}')\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.25))\n",
    "                column_names.append(f'Q1_{val}_{part}')\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.75))\n",
    "                column_names.append(f'Q3_{val}_{part}')\n",
    "                start = start + size\n",
    "                part += 1\n",
    "        row.append(label)\n",
    "        rows.append(row[:])\n",
    "    \n",
    "    column_names.append('Label')\n",
    "    nb_df = pd.DataFrame(rows, columns = column_names)\n",
    "    nb = GaussianNB()\n",
    "    model = nb.fit(nb_df.iloc[:,:-1], nb_df.iloc[:,-1])\n",
    "    score = model.score(nb_df.iloc[:,:-1], nb_df.iloc[:,-1])\n",
    "    if score > naive_best_score:\n",
    "        naive_best_score = score\n",
    "        naive_best_model = model\n",
    "#         multi_best_c = model.C_\n",
    "        naive_best_l = l\n",
    "        naive_best_train_df = nb_df.copy()\n",
    "\n",
    "print(f'Best l is {naive_best_l}')\n",
    "# print(f'Best Cs are {multi_best_c}')\n",
    "print(naive_best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a0b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data\n",
    "rows = []\n",
    "for df, path in zip(testing_dfs, testing_paths):\n",
    "    size = math.ceil(len(df) / naive_best_l)\n",
    "    label = path.split('/')[2]\n",
    "    row = []\n",
    "    column_names = []\n",
    "    for val in column_nums:\n",
    "        start = 0\n",
    "        part = 1\n",
    "        current = np.reshape(df.iloc[:,val-1].astype(float).values, (-1,1))\n",
    "        while start < len(df):\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).mean())\n",
    "            column_names.append(f'Mean_{val}_{part}')\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.25))\n",
    "            column_names.append(f'Q1_{val}_{part}')\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.75))\n",
    "            column_names.append(f'Q3_{val}_{part}')\n",
    "            start = start + size\n",
    "            part += 1\n",
    "    row.append(label)\n",
    "    rows.append(row[:])\n",
    "\n",
    "column_names.append('Label')\n",
    "naive_test_df = pd.DataFrame(rows, columns = column_names)\n",
    "naive_test_df.shape\n",
    "\n",
    "X_test_df_naive = naive_test_df.iloc[:,:-1]\n",
    "y_test_df_naive = naive_test_df.iloc[:, -1]\n",
    "preds = naive_best_model.predict(X_test_df_naive)\n",
    "score = naive_best_model.score(X_test_df_naive, y_test_df_naive)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91898fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = metrics.confusion_matrix(y_test_df_naive, preds)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_nums = [1,2,6] # take col - 1 for proper indexing\n",
    "naive_best_score = float('-inf') \n",
    "for l in range(1, 21):\n",
    "    rows = []\n",
    "    for df, path in zip(training_dfs, training_paths):\n",
    "        size = math.ceil(len(df) / l)\n",
    "        label = path.split('/')[2]\n",
    "        row = []\n",
    "        column_names = []\n",
    "        for val in column_nums:\n",
    "            start = 0\n",
    "            part = 1\n",
    "            while start < len(df):\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).mean())\n",
    "                column_names.append(f'Mean_{val}_{part}')\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.25))\n",
    "                column_names.append(f'Q1_{val}_{part}')\n",
    "                row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.75))\n",
    "                column_names.append(f'Q3_{val}_{part}')\n",
    "                start = start + size\n",
    "                part += 1\n",
    "        row.append(label)\n",
    "        rows.append(row[:])\n",
    "    \n",
    "    column_names.append('Label')\n",
    "    nb_df = pd.DataFrame(rows, columns = column_names)\n",
    "    nb = MultinomialNB()\n",
    "    model = nb.fit(nb_df.iloc[:,:-1], nb_df.iloc[:,-1])\n",
    "    score = model.score(nb_df.iloc[:,:-1], nb_df.iloc[:,-1])\n",
    "    if score > naive_best_score:\n",
    "        naive_best_score = score\n",
    "        naive_best_model = model\n",
    "#         multi_best_c = model.C_\n",
    "        naive_best_l = l\n",
    "        naive_best_train_df = nb_df.copy()\n",
    "\n",
    "print(f'Best l is {naive_best_l}')\n",
    "# print(f'Best Cs are {multi_best_c}')\n",
    "print(naive_best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data\n",
    "rows = []\n",
    "for df, path in zip(testing_dfs, testing_paths):\n",
    "    size = math.ceil(len(df) / naive_best_l)\n",
    "    label = path.split('/')[2]\n",
    "    row = []\n",
    "    column_names = []\n",
    "    for val in column_nums:\n",
    "        start = 0\n",
    "        part = 1\n",
    "        current = np.reshape(df.iloc[:,val-1].astype(float).values, (-1,1))\n",
    "        while start < len(df):\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).mean())\n",
    "            column_names.append(f'Mean_{val}_{part}')\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.25))\n",
    "            column_names.append(f'Q1_{val}_{part}')\n",
    "            row.append(df[df.columns[val-1]].iloc[start:start+size].astype(float).quantile(0.75))\n",
    "            column_names.append(f'Q3_{val}_{part}')\n",
    "            start = start + size\n",
    "            part += 1\n",
    "    row.append(label)\n",
    "    rows.append(row[:])\n",
    "\n",
    "column_names.append('Label')\n",
    "naive_test_df = pd.DataFrame(rows, columns = column_names)\n",
    "naive_test_df.shape\n",
    "\n",
    "X_test_df_naive = naive_test_df.iloc[:,:-1]\n",
    "y_test_df_naive = naive_test_df.iloc[:, -1]\n",
    "preds = naive_best_model.predict(X_test_df_naive)\n",
    "score = naive_best_model.score(X_test_df_naive, y_test_df_naive)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bba8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = metrics.confusion_matrix(y_test_df_naive, preds)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa55cde",
   "metadata": {},
   "source": [
    "### iii. Which method is better for multi-class classification in this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae33086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f995f4f",
   "metadata": {},
   "source": [
    "# 3. ISLR, 4.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0727b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83701ee6",
   "metadata": {},
   "source": [
    "# 4. ISLR 4.8.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9b464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
