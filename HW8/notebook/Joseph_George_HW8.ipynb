{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4551bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, auc, roc_curve\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d499b",
   "metadata": {},
   "source": [
    "# 1. Supervised, Semi-Supervised, and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87cd98",
   "metadata": {},
   "source": [
    "## (a) Download the Breast Cancer Wisconsin (Diagnostic) Data Set from: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+ %28Diagnostic%29. Download the data in https://archive.ics.uci.edu/ml/ machine-learning-databases/breast-cancer-wisconsin/wdbc.data, which has IDs, classes (Benign=B, Malignant=M), and 30 attributes. This data has two output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef1aa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>...</td>\n",
       "      <td>15.47</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>...</td>\n",
       "      <td>17.06</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M</td>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>...</td>\n",
       "      <td>15.49</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>M</td>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>...</td>\n",
       "      <td>15.09</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  1      2      3       4       5        6        7        8        9   \\\n",
       "0  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710   \n",
       "1  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017   \n",
       "2  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790   \n",
       "3  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520   \n",
       "4  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430   \n",
       "5  M  12.45  15.70   82.57   477.1  0.12780  0.17000  0.15780  0.08089   \n",
       "6  M  18.25  19.98  119.60  1040.0  0.09463  0.10900  0.11270  0.07400   \n",
       "7  M  13.71  20.83   90.20   577.9  0.11890  0.16450  0.09366  0.05985   \n",
       "8  M  13.00  21.82   87.50   519.8  0.12730  0.19320  0.18590  0.09353   \n",
       "9  M  12.46  24.04   83.97   475.9  0.11860  0.23960  0.22730  0.08543   \n",
       "\n",
       "       10  ...     22     23      24      25      26      27      28      29  \\\n",
       "0  0.2419  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.1812  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.2069  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.2597  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.1809  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "5  0.2087  ...  15.47  23.75  103.40   741.6  0.1791  0.5249  0.5355  0.1741   \n",
       "6  0.1794  ...  22.88  27.66  153.20  1606.0  0.1442  0.2576  0.3784  0.1932   \n",
       "7  0.2196  ...  17.06  28.14  110.60   897.0  0.1654  0.3682  0.2678  0.1556   \n",
       "8  0.2350  ...  15.49  30.73  106.20   739.3  0.1703  0.5401  0.5390  0.2060   \n",
       "9  0.2030  ...  15.09  40.68   97.65   711.4  0.1853  1.0580  1.1050  0.2210   \n",
       "\n",
       "       30       31  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "5  0.3985  0.12440  \n",
       "6  0.3063  0.08368  \n",
       "7  0.3196  0.11510  \n",
       "8  0.4378  0.10720  \n",
       "9  0.4366  0.20750  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/wdbc.data\", header = None, index_col = False)\n",
    "df = df.iloc[:,1:]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb6bfee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B    357\n",
       "M    212\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2882a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360f207",
   "metadata": {},
   "source": [
    "## (b) Monte-Carlo Simulation: Repeat the following procedures for supervised, un- supervised, and semi-supervised learning M = 30 times, and use randomly se- lected train and test data (make sure you use 20% of both the positve and nega- tive classes as the test set). Then compare the average scores (accuracy, precision, recall, F1-score, and AUC) that you obtain from each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2784c7",
   "metadata": {},
   "source": [
    "### i. Supervised Learning: Train an L1-penalized SVM to classify the data. Use 5 fold cross validation to choose the penalty parameter. Use normalized data. Report the average accuracy, precision, recall, F1-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43febe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.iloc[:,1:], df.iloc[:,0]\n",
    "LE = LabelEncoder()\n",
    "y = LE.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6675c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 72, 1: 42})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76c1fbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>...</td>\n",
       "      <td>10.490</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.1073</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.06969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>12.630</td>\n",
       "      <td>20.76</td>\n",
       "      <td>82.15</td>\n",
       "      <td>480.4</td>\n",
       "      <td>0.09933</td>\n",
       "      <td>0.12090</td>\n",
       "      <td>0.10650</td>\n",
       "      <td>0.06021</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>0.07070</td>\n",
       "      <td>...</td>\n",
       "      <td>13.330</td>\n",
       "      <td>25.47</td>\n",
       "      <td>89.00</td>\n",
       "      <td>527.4</td>\n",
       "      <td>0.1287</td>\n",
       "      <td>0.22500</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>7.729</td>\n",
       "      <td>25.49</td>\n",
       "      <td>47.98</td>\n",
       "      <td>178.8</td>\n",
       "      <td>0.08098</td>\n",
       "      <td>0.04878</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>0.07285</td>\n",
       "      <td>...</td>\n",
       "      <td>9.077</td>\n",
       "      <td>30.92</td>\n",
       "      <td>57.17</td>\n",
       "      <td>248.0</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.08340</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.3058</td>\n",
       "      <td>0.09938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>12.180</td>\n",
       "      <td>17.84</td>\n",
       "      <td>77.79</td>\n",
       "      <td>451.1</td>\n",
       "      <td>0.10450</td>\n",
       "      <td>0.07057</td>\n",
       "      <td>0.02490</td>\n",
       "      <td>0.02941</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.06635</td>\n",
       "      <td>...</td>\n",
       "      <td>12.830</td>\n",
       "      <td>20.92</td>\n",
       "      <td>82.14</td>\n",
       "      <td>495.2</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.09358</td>\n",
       "      <td>0.0498</td>\n",
       "      <td>0.05882</td>\n",
       "      <td>0.2227</td>\n",
       "      <td>0.07376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>13.240</td>\n",
       "      <td>20.13</td>\n",
       "      <td>86.87</td>\n",
       "      <td>542.9</td>\n",
       "      <td>0.08284</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.02833</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.06432</td>\n",
       "      <td>...</td>\n",
       "      <td>15.440</td>\n",
       "      <td>25.50</td>\n",
       "      <td>115.00</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.56460</td>\n",
       "      <td>0.6556</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.2845</td>\n",
       "      <td>0.12490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>21.100</td>\n",
       "      <td>20.52</td>\n",
       "      <td>138.10</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>0.09684</td>\n",
       "      <td>0.11750</td>\n",
       "      <td>0.15720</td>\n",
       "      <td>0.11550</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>...</td>\n",
       "      <td>25.680</td>\n",
       "      <td>32.07</td>\n",
       "      <td>168.20</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>0.31010</td>\n",
       "      <td>0.4399</td>\n",
       "      <td>0.22800</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.07425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>21.750</td>\n",
       "      <td>20.99</td>\n",
       "      <td>147.30</td>\n",
       "      <td>1491.0</td>\n",
       "      <td>0.09401</td>\n",
       "      <td>0.19610</td>\n",
       "      <td>0.21950</td>\n",
       "      <td>0.10880</td>\n",
       "      <td>0.1721</td>\n",
       "      <td>0.06194</td>\n",
       "      <td>...</td>\n",
       "      <td>28.190</td>\n",
       "      <td>28.18</td>\n",
       "      <td>195.90</td>\n",
       "      <td>2384.0</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.47250</td>\n",
       "      <td>0.5807</td>\n",
       "      <td>0.18410</td>\n",
       "      <td>0.2833</td>\n",
       "      <td>0.08858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>12.160</td>\n",
       "      <td>18.03</td>\n",
       "      <td>78.29</td>\n",
       "      <td>455.3</td>\n",
       "      <td>0.09087</td>\n",
       "      <td>0.07838</td>\n",
       "      <td>0.02916</td>\n",
       "      <td>0.01527</td>\n",
       "      <td>0.1464</td>\n",
       "      <td>0.06284</td>\n",
       "      <td>...</td>\n",
       "      <td>13.340</td>\n",
       "      <td>27.87</td>\n",
       "      <td>88.83</td>\n",
       "      <td>547.4</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>0.22790</td>\n",
       "      <td>0.1620</td>\n",
       "      <td>0.05690</td>\n",
       "      <td>0.2406</td>\n",
       "      <td>0.07729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>12.760</td>\n",
       "      <td>13.37</td>\n",
       "      <td>82.29</td>\n",
       "      <td>504.1</td>\n",
       "      <td>0.08794</td>\n",
       "      <td>0.07948</td>\n",
       "      <td>0.04052</td>\n",
       "      <td>0.02548</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.06140</td>\n",
       "      <td>...</td>\n",
       "      <td>14.190</td>\n",
       "      <td>16.40</td>\n",
       "      <td>92.04</td>\n",
       "      <td>618.8</td>\n",
       "      <td>0.1194</td>\n",
       "      <td>0.22080</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.08411</td>\n",
       "      <td>0.2564</td>\n",
       "      <td>0.08253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>15.750</td>\n",
       "      <td>19.22</td>\n",
       "      <td>107.10</td>\n",
       "      <td>758.6</td>\n",
       "      <td>0.12430</td>\n",
       "      <td>0.23640</td>\n",
       "      <td>0.29140</td>\n",
       "      <td>0.12420</td>\n",
       "      <td>0.2375</td>\n",
       "      <td>0.07603</td>\n",
       "      <td>...</td>\n",
       "      <td>17.360</td>\n",
       "      <td>24.17</td>\n",
       "      <td>119.40</td>\n",
       "      <td>915.3</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.50460</td>\n",
       "      <td>0.6872</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.10500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         2      3       4       5        6        7        8        9   \\\n",
       "557   9.423  27.88   59.26   271.3  0.08123  0.04971  0.00000  0.00000   \n",
       "111  12.630  20.76   82.15   480.4  0.09933  0.12090  0.10650  0.06021   \n",
       "538   7.729  25.49   47.98   178.8  0.08098  0.04878  0.00000  0.00000   \n",
       "96   12.180  17.84   77.79   451.1  0.10450  0.07057  0.02490  0.02941   \n",
       "465  13.240  20.13   86.87   542.9  0.08284  0.12230  0.10100  0.02833   \n",
       "449  21.100  20.52  138.10  1384.0  0.09684  0.11750  0.15720  0.11550   \n",
       "272  21.750  20.99  147.30  1491.0  0.09401  0.19610  0.21950  0.10880   \n",
       "480  12.160  18.03   78.29   455.3  0.09087  0.07838  0.02916  0.01527   \n",
       "312  12.760  13.37   82.29   504.1  0.08794  0.07948  0.04052  0.02548   \n",
       "351  15.750  19.22  107.10   758.6  0.12430  0.23640  0.29140  0.12420   \n",
       "\n",
       "         10       11  ...      22     23      24      25      26       27  \\\n",
       "557  0.1742  0.06059  ...  10.490  34.24   66.50   330.6  0.1073  0.07158   \n",
       "111  0.1735  0.07070  ...  13.330  25.47   89.00   527.4  0.1287  0.22500   \n",
       "538  0.1870  0.07285  ...   9.077  30.92   57.17   248.0  0.1256  0.08340   \n",
       "96   0.1900  0.06635  ...  12.830  20.92   82.14   495.2  0.1140  0.09358   \n",
       "465  0.1601  0.06432  ...  15.440  25.50  115.00   733.5  0.1201  0.56460   \n",
       "449  0.1554  0.05661  ...  25.680  32.07  168.20  2022.0  0.1368  0.31010   \n",
       "272  0.1721  0.06194  ...  28.190  28.18  195.90  2384.0  0.1272  0.47250   \n",
       "480  0.1464  0.06284  ...  13.340  27.87   88.83   547.4  0.1208  0.22790   \n",
       "312  0.1601  0.06140  ...  14.190  16.40   92.04   618.8  0.1194  0.22080   \n",
       "351  0.2375  0.07603  ...  17.360  24.17  119.40   915.3  0.1550  0.50460   \n",
       "\n",
       "         28       29      30       31  \n",
       "557  0.0000  0.00000  0.2475  0.06969  \n",
       "111  0.2216  0.11050  0.2226  0.08486  \n",
       "538  0.0000  0.00000  0.3058  0.09938  \n",
       "96   0.0498  0.05882  0.2227  0.07376  \n",
       "465  0.6556  0.13570  0.2845  0.12490  \n",
       "449  0.4399  0.22800  0.2268  0.07425  \n",
       "272  0.5807  0.18410  0.2833  0.08858  \n",
       "480  0.1620  0.05690  0.2406  0.07729  \n",
       "312  0.1769  0.08411  0.2564  0.08253  \n",
       "351  0.6872  0.21350  0.4245  0.10500  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8817da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                                       (&#x27;SVM&#x27;,\n",
       "                                        LinearSVC(dual=False, penalty=&#x27;l1&#x27;,\n",
       "                                                  tol=0.01))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;SVM__C&#x27;: array([1.00000000e-10, 1.12883789e-09, 1.27427499e-08, 1.43844989e-07,\n",
       "       1.62377674e-06, 1.83298071e-05, 2.06913808e-04, 2.33572147e-03,\n",
       "       2.63665090e-02, 2.97635144e-01, 3.35981829e+00, 3.79269019e+01,\n",
       "       4.28133240e+02, 4.83293024e+03, 5.45559478e+04, 6.15848211e+05,\n",
       "       6.95192796e+06, 7.84759970e+07, 8.85866790e+08, 1.00000000e+10])},\n",
       "             return_train_score=True, scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                                       (&#x27;SVM&#x27;,\n",
       "                                        LinearSVC(dual=False, penalty=&#x27;l1&#x27;,\n",
       "                                                  tol=0.01))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;SVM__C&#x27;: array([1.00000000e-10, 1.12883789e-09, 1.27427499e-08, 1.43844989e-07,\n",
       "       1.62377674e-06, 1.83298071e-05, 2.06913808e-04, 2.33572147e-03,\n",
       "       2.63665090e-02, 2.97635144e-01, 3.35981829e+00, 3.79269019e+01,\n",
       "       4.28133240e+02, 4.83293024e+03, 5.45559478e+04, 6.15848211e+05,\n",
       "       6.95192796e+06, 7.84759970e+07, 8.85866790e+08, 1.00000000e+10])},\n",
       "             return_train_score=True, scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;SVM&#x27;, LinearSVC(dual=False, penalty=&#x27;l1&#x27;, tol=0.01))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(dual=False, penalty=&#x27;l1&#x27;, tol=0.01)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', MinMaxScaler()),\n",
       "                                       ('SVM',\n",
       "                                        LinearSVC(dual=False, penalty='l1',\n",
       "                                                  tol=0.01))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'SVM__C': array([1.00000000e-10, 1.12883789e-09, 1.27427499e-08, 1.43844989e-07,\n",
       "       1.62377674e-06, 1.83298071e-05, 2.06913808e-04, 2.33572147e-03,\n",
       "       2.63665090e-02, 2.97635144e-01, 3.35981829e+00, 3.79269019e+01,\n",
       "       4.28133240e+02, 4.83293024e+03, 5.45559478e+04, 6.15848211e+05,\n",
       "       6.95192796e+06, 7.84759970e+07, 8.85866790e+08, 1.00000000e+10])},\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('scaler', MinMaxScaler()),\n",
    "         ('SVM',  LinearSVC(penalty='l1', dual=False, tol=1e-2))]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "parameters = {'SVM__C': np.logspace(-10, 10, 20)}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, cv = 5, scoring='accuracy',return_train_score=True, refit=True, n_jobs=-1)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a0ea78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736263736263737\n",
      "{'SVM__C': 3.359818286283774}\n"
     ]
    }
   ],
   "source": [
    "best_model = cv.best_estimator_\n",
    "print(cv.best_score_)\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d24235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.7648351648351648\n",
      "Counter({0: 392, 1: 63})\n",
      "Counter({0: 285, 1: 170})\n",
      "Train Precision 0.37058823529411766\n",
      "Train Recall Score 1.0\n",
      "Train F1 Score 0.5407725321888412\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "ypreds = best_model.predict(X_train_scaled)\n",
    "yhat = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Train Accuracy\", accuracy_score(ypreds, y_train))\n",
    "print(Counter(ypreds))\n",
    "print(Counter(y_train))\n",
    "print(\"Train Precision\", precision_score(ypreds, y_train))\n",
    "print(\"Train Recall Score\", recall_score(ypreds, y_train))\n",
    "print(\"Train F1 Score\", f1_score(ypreds, y_train))\n",
    "roc_auc_c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aee876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
